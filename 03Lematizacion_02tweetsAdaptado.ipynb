{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bda7f38",
   "metadata": {},
   "source": [
    "# ‚úÖ Lematizaci√≥n Simple (sin POS tagging) para Tweets en Espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2e646d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rodrigovillacinda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rodrigovillacinda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/rodrigovillacinda/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üì¶ IMPORTACI√ìN DE LIBRER√çAS Y RECURSOS\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "# Descargas necesarias\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4bac566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ DEFINICI√ìN DE FUNCI√ìN DE LIMPIEZA Y LEMATIZACI√ìN SIMPLE\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def limpiar_y_lematizar(text_limpio):\n",
    "    if not isinstance(text_limpio, str):\n",
    "        return \"\"\n",
    "    text_limpio = text_limpio.lower()\n",
    "    text_limpio = text_limpio.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = tokenizer.tokenize(text_limpio)\n",
    "    lematizado = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(lematizado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b785abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataset: Index(['textID', 'text', 'selected_text', 'sentiment', 'text_limpio',\n",
      "       'tokens'],\n",
      "      dtype='object')\n",
      "‚úÖ Archivo '03lemmatized.csv' generado con √©xito.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_limpio</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text_limpio_limpio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>idhaverespondedifiweregoing</td>\n",
       "      <td>['idhaverespondedifiweregoing']</td>\n",
       "      <td>idhaverespondedifiweregoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooosadiwillmissyouhereinsandiego</td>\n",
       "      <td>['sooosadiwillmissyouhereinsandiego']</td>\n",
       "      <td>sooosadiwillmissyouhereinsandiego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>mybossisbullyingme</td>\n",
       "      <td>['mybossisbullyingme']</td>\n",
       "      <td>mybossisbullyingme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>whatinterviewleavemealone</td>\n",
       "      <td>['whatinterviewleavemealone']</td>\n",
       "      <td>whatinterviewleavemealone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>sonsofwhycouldnttheyputthemonthereleaseswealre...</td>\n",
       "      <td>['sonsofwhycouldnttheyputthemonthereleasesweal...</td>\n",
       "      <td>sonsofwhycouldnttheyputthemonthereleaseswealre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>httpwwwdothebouncycomsmfsomeshamelesspluggingf...</td>\n",
       "      <td>['httpwwwdothebouncycomsmfsomeshamelesspluggin...</td>\n",
       "      <td>httpwwwdothebouncycomsmfsomeshamelesspluggingf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>2amfeedingsforthebabyarefunwhenheisallsmilesan...</td>\n",
       "      <td>['2amfeedingsforthebabyarefunwhenheisallsmiles...</td>\n",
       "      <td>2amfeedingsforthebabyarefunwhenheisallsmilesan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>soooohigh</td>\n",
       "      <td>['soooohigh']</td>\n",
       "      <td>soooohigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>bothofyou</td>\n",
       "      <td>['bothofyou']</td>\n",
       "      <td>bothofyou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>journeywowujustbecamecoolerheheisthatpossible</td>\n",
       "      <td>['journeywowujustbecamecoolerheheisthatpossible']</td>\n",
       "      <td>journeywowujustbecamecoolerheheisthatpossible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \\\n",
       "0                I`d have responded, if I were going   neutral   \n",
       "1                                           Sooo SAD  negative   \n",
       "2                                        bullying me  negative   \n",
       "3                                     leave me alone  negative   \n",
       "4                                      Sons of ****,  negative   \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral   \n",
       "6                                                fun  positive   \n",
       "7                                         Soooo high   neutral   \n",
       "8                                        Both of you   neutral   \n",
       "9                       Wow... u just became cooler.  positive   \n",
       "\n",
       "                                         text_limpio  \\\n",
       "0                        idhaverespondedifiweregoing   \n",
       "1                  sooosadiwillmissyouhereinsandiego   \n",
       "2                                 mybossisbullyingme   \n",
       "3                          whatinterviewleavemealone   \n",
       "4  sonsofwhycouldnttheyputthemonthereleaseswealre...   \n",
       "5  httpwwwdothebouncycomsmfsomeshamelesspluggingf...   \n",
       "6  2amfeedingsforthebabyarefunwhenheisallsmilesan...   \n",
       "7                                          soooohigh   \n",
       "8                                          bothofyou   \n",
       "9      journeywowujustbecamecoolerheheisthatpossible   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                    ['idhaverespondedifiweregoing']   \n",
       "1              ['sooosadiwillmissyouhereinsandiego']   \n",
       "2                             ['mybossisbullyingme']   \n",
       "3                      ['whatinterviewleavemealone']   \n",
       "4  ['sonsofwhycouldnttheyputthemonthereleasesweal...   \n",
       "5  ['httpwwwdothebouncycomsmfsomeshamelesspluggin...   \n",
       "6  ['2amfeedingsforthebabyarefunwhenheisallsmiles...   \n",
       "7                                      ['soooohigh']   \n",
       "8                                      ['bothofyou']   \n",
       "9  ['journeywowujustbecamecoolerheheisthatpossible']   \n",
       "\n",
       "                                  text_limpio_limpio  \n",
       "0                        idhaverespondedifiweregoing  \n",
       "1                  sooosadiwillmissyouhereinsandiego  \n",
       "2                                 mybossisbullyingme  \n",
       "3                          whatinterviewleavemealone  \n",
       "4  sonsofwhycouldnttheyputthemonthereleaseswealre...  \n",
       "5  httpwwwdothebouncycomsmfsomeshamelesspluggingf...  \n",
       "6  2amfeedingsforthebabyarefunwhenheisallsmilesan...  \n",
       "7                                          soooohigh  \n",
       "8                                          bothofyou  \n",
       "9      journeywowujustbecamecoolerheheisthatpossible  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üìÇ CARGA DE DATOS Y APLICACI√ìN\n",
    "df = pd.read_csv('02tweetsTokenizados.csv')\n",
    "print(\"Columnas del dataset:\", df.columns)\n",
    "\n",
    "# Aplica lematizaci√≥n a la columna 'text_limpio'\n",
    "df['text_limpio_limpio'] = df['text_limpio'].apply(limpiar_y_lematizar)\n",
    "\n",
    "# Guarda el resultado\n",
    "df.to_csv('03lemmatized.csv', index=False)\n",
    "print(\"‚úÖ Archivo '03lemmatized.csv' generado con √©xito.\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadaec6-ad77-43ec-8037-3289e0e932e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb436017-c370-440b-94fe-f2c72c87d096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
